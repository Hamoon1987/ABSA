{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Pars-BERT-NLI-M.ipynb","provenance":[{"file_id":"15o7r0_HWV-o-B45GBFJ9ENScr0Diluc_","timestamp":1603441158367},{"file_id":"1T0XY2QEMzADbFmixghX4894tm9Ii6cE6","timestamp":1603389406062},{"file_id":"1H89xGIeoykL1CKlQt2VWw8-1IeS-gxYl","timestamp":1603189335942},{"file_id":"1JH1iUNbdAbd7Fb_rq6FNQdt057XhUW36","timestamp":1602151388427}],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPmd1CLi9/MP00WH4E1mBtz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"3d6dbbbb05384d17afc7a2196f22d449":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_dc977ff09c0a48b6bfbc62ebf1619291","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_cf879784c3414732acd3b1e2a0de8aff","IPY_MODEL_27ad91b315be48c5a93befcce73f3b01"]}},"dc977ff09c0a48b6bfbc62ebf1619291":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cf879784c3414732acd3b1e2a0de8aff":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9637dcb6fb3a40c6ac2013b42efe7f37","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":440,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":440,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_353e585f8d5e4c53bc85d05960b55f64"}},"27ad91b315be48c5a93befcce73f3b01":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_af6cf862d5f64ad6a8c105e13a49d7a2","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 440/440 [00:06&lt;00:00, 65.7B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_88f84366df0e40b0b21bc8c02e7f8a98"}},"9637dcb6fb3a40c6ac2013b42efe7f37":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"353e585f8d5e4c53bc85d05960b55f64":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"af6cf862d5f64ad6a8c105e13a49d7a2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"88f84366df0e40b0b21bc8c02e7f8a98":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c3e78b920c934cd7bf8992a2e641acab":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_db7d12af119143d3b054ae08b68cc952","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_510c1088def341e284f47ddc2e0267df","IPY_MODEL_fce37cb4ed4440e8bdd8c4bd127b502b"]}},"db7d12af119143d3b054ae08b68cc952":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"510c1088def341e284f47ddc2e0267df":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b7d376bf53464d03bb61cc78e0751d21","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1198122,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1198122,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1602b8a37e454a168a79fd01d4768778"}},"fce37cb4ed4440e8bdd8c4bd127b502b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0a0ea1dc3bfa4050a8d4fef93b50be7f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.20M/1.20M [00:05&lt;00:00, 237kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_47041faf7e4b4d5ca2527b8ccea2ecc3"}},"b7d376bf53464d03bb61cc78e0751d21":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1602b8a37e454a168a79fd01d4768778":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0a0ea1dc3bfa4050a8d4fef93b50be7f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"47041faf7e4b4d5ca2527b8ccea2ecc3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4fb7a40109734a89b25c620bbe8fccad":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3af284c1ea7e4fd8a0a12b556d3cc38d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_45fd2eba7ce1456e97e8adfc5b2e4f13","IPY_MODEL_dec1b0a142354178a111864c971b6213"]}},"3af284c1ea7e4fd8a0a12b556d3cc38d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"45fd2eba7ce1456e97e8adfc5b2e4f13":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_8f14fe9d3ceb4b39a4033728aa28f6c4","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":654226731,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":654226731,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d53f53b5586248d0bb1f46b58f316d77"}},"dec1b0a142354178a111864c971b6213":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c21cf7403fc8485fa9b70366799c21ec","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 654M/654M [01:04&lt;00:00, 10.2MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_eb6be1a585304bdb9fed0e2901afdb9e"}},"8f14fe9d3ceb4b39a4033728aa28f6c4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d53f53b5586248d0bb1f46b58f316d77":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c21cf7403fc8485fa9b70366799c21ec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"eb6be1a585304bdb9fed0e2901afdb9e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"boJxEidx_vnS"},"source":["### Importing Data"]},{"cell_type":"code","metadata":{"id":"s8py1sA47dVw","executionInfo":{"status":"ok","timestamp":1606311233237,"user_tz":-210,"elapsed":2045,"user":{"displayName":"Hamoon jafarian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQqWT2-N8jq2Rj1MiVOscMiLQtxuqgyz6IZJcxPQ=s64","userId":"16895573338538093748"}}},"source":["from xml.dom import minidom\n","import pandas as pd"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"7kVLT0q-7iHq","executionInfo":{"status":"ok","timestamp":1606311235965,"user_tz":-210,"elapsed":4754,"user":{"displayName":"Hamoon jafarian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQqWT2-N8jq2Rj1MiVOscMiLQtxuqgyz6IZJcxPQ=s64","userId":"16895573338538093748"}}},"source":["# Reading xml dataset\n","xmldoc = minidom.parse('/content/dataxml.xml')\n","sentences = xmldoc.getElementsByTagName(\"sentence\")\n","df = pd.DataFrame()\n","sentence_list = []\n","aspect_list = []\n","polarity_list = []\n","for sentence in sentences:\n","  for i in range(len(sentence.getElementsByTagName(\"aspectTerm\"))):\n","    sentence_list.append(sentence.getElementsByTagName(\"text\")[0].firstChild.data)\n","    aspect_list.append(sentence.getElementsByTagName(\"aspectTerm\")[i].getAttribute('term'))\n","    polarity_list.append(sentence.getElementsByTagName(\"aspectTerm\")[i].getAttribute('polarity'))"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"_bbaXC2WiRwp","executionInfo":{"status":"ok","timestamp":1606311235968,"user_tz":-210,"elapsed":4744,"user":{"displayName":"Hamoon jafarian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQqWT2-N8jq2Rj1MiVOscMiLQtxuqgyz6IZJcxPQ=s64","userId":"16895573338538093748"}}},"source":["# Forming DataFrame\n","df['sentence'] = sentence_list\n","df['aspect'] = aspect_list\n","df['polarity'] = polarity_list"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jPR7fgxhpVl0"},"source":["### Installing the Hugging Face Library"]},{"cell_type":"code","metadata":{"id":"C32X2_RFpVl4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606311244138,"user_tz":-210,"elapsed":12904,"user":{"displayName":"Hamoon jafarian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQqWT2-N8jq2Rj1MiVOscMiLQtxuqgyz6IZJcxPQ=s64","userId":"16895573338538093748"}},"outputId":"c32fb4ad-cbe4-4513-ebbd-d11970843ceb"},"source":["!pip install transformers"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n","\u001b[K     |████████████████████████████████| 1.3MB 5.5MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Collecting tokenizers==0.9.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 19.4MB/s \n","\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 41.3MB/s \n","\u001b[?25hCollecting sentencepiece==0.1.91\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 37.7MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=9c55629740d23a426305feca68fb8df6143099fe954e0b375334398a4e49e5d8\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Pm6EtoEhpVnC"},"source":["### Limit comments length"]},{"cell_type":"code","metadata":{"id":"QaFISPEOpVnF","colab":{"base_uri":"https://localhost:8080/","height":118,"referenced_widgets":["3d6dbbbb05384d17afc7a2196f22d449","dc977ff09c0a48b6bfbc62ebf1619291","cf879784c3414732acd3b1e2a0de8aff","27ad91b315be48c5a93befcce73f3b01","9637dcb6fb3a40c6ac2013b42efe7f37","353e585f8d5e4c53bc85d05960b55f64","af6cf862d5f64ad6a8c105e13a49d7a2","88f84366df0e40b0b21bc8c02e7f8a98","c3e78b920c934cd7bf8992a2e641acab","db7d12af119143d3b054ae08b68cc952","510c1088def341e284f47ddc2e0267df","fce37cb4ed4440e8bdd8c4bd127b502b","b7d376bf53464d03bb61cc78e0751d21","1602b8a37e454a168a79fd01d4768778","0a0ea1dc3bfa4050a8d4fef93b50be7f","47041faf7e4b4d5ca2527b8ccea2ecc3"]},"executionInfo":{"status":"ok","timestamp":1606311259600,"user_tz":-210,"elapsed":28357,"user":{"displayName":"Hamoon jafarian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQqWT2-N8jq2Rj1MiVOscMiLQtxuqgyz6IZJcxPQ=s64","userId":"16895573338538093748"}},"outputId":"8e112913-a9cd-4032-a464-a684129a42f6"},"source":["from transformers import AutoTokenizer\n","# Using Pars-BERT Tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"HooshvareLab/bert-fa-base-uncased\")\n","\n","# Using Multilingual BERT Tokenizer\n","# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")"],"execution_count":7,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3d6dbbbb05384d17afc7a2196f22d449","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c3e78b920c934cd7bf8992a2e641acab","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1198122.0, style=ProgressStyle(descript…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xRNsrq8vsnE1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606311274085,"user_tz":-210,"elapsed":42832,"user":{"displayName":"Hamoon jafarian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQqWT2-N8jq2Rj1MiVOscMiLQtxuqgyz6IZJcxPQ=s64","userId":"16895573338538093748"}},"outputId":"2ce0b07f-df6d-419b-a9cf-e395d165f956"},"source":["# Calculate the number of tokens in each comment\n","df['len_sentence'] = df['sentence'].apply(lambda t: len(tokenizer.tokenize(t)))\n","print(df['len_sentence'].max())\n","print(df['len_sentence'].min())"],"execution_count":8,"outputs":[{"output_type":"stream","text":["1248\n","2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rnCj-iLe3cjb","executionInfo":{"status":"ok","timestamp":1606311274095,"user_tz":-210,"elapsed":42834,"user":{"displayName":"Hamoon jafarian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQqWT2-N8jq2Rj1MiVOscMiLQtxuqgyz6IZJcxPQ=s64","userId":"16895573338538093748"}}},"source":["# Finding the share of comments with a specific length\n","def data_gl_than(data, less_than=100.0, greater_than=0.0, col='len_sentence'):\n","    data_length = data[col].values\n","\n","    data_glt = sum([1 for length in data_length if greater_than < length <= less_than])\n","\n","    data_glt_rate = (data_glt / len(data_length)) * 100\n","\n","    print(f'Texts with word length of greater than {greater_than} and less than {less_than} includes {data_glt_rate:.2f}% of the whole!')"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"iket7VRN5cgG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606311274098,"user_tz":-210,"elapsed":42830,"user":{"displayName":"Hamoon jafarian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQqWT2-N8jq2Rj1MiVOscMiLQtxuqgyz6IZJcxPQ=s64","userId":"16895573338538093748"}},"outputId":"4947804a-675b-43b9-daa4-daa714710b82"},"source":["data_gl_than(df, 256, 1)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Texts with word length of greater than 1 and less than 256 includes 97.51% of the whole!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kNausH6L55fQ","executionInfo":{"status":"ok","timestamp":1606311274105,"user_tz":-210,"elapsed":42828,"user":{"displayName":"Hamoon jafarian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQqWT2-N8jq2Rj1MiVOscMiLQtxuqgyz6IZJcxPQ=s64","userId":"16895573338538093748"}}},"source":["# Set min and max limits of length\n","minlim, maxlim = 1, 256"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"kliWRX7-3U1c","executionInfo":{"status":"ok","timestamp":1606311274108,"user_tz":-210,"elapsed":42821,"user":{"displayName":"Hamoon jafarian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQqWT2-N8jq2Rj1MiVOscMiLQtxuqgyz6IZJcxPQ=s64","userId":"16895573338538093748"}}},"source":["# remove comments with the length of fewer than three words\n","df['len_sentence'] = df['len_sentence'].apply(lambda len_t: len_t if minlim < len_t <= maxlim else None)\n","df = df.dropna(subset=['len_sentence'])\n","df = df.reset_index(drop=True)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"4NzPlyFtub6w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606311274110,"user_tz":-210,"elapsed":42813,"user":{"displayName":"Hamoon jafarian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQqWT2-N8jq2Rj1MiVOscMiLQtxuqgyz6IZJcxPQ=s64","userId":"16895573338538093748"}},"outputId":"f73a8355-daf3-4d5c-9702-5751ea2dd43c"},"source":["print(df['len_sentence'].max())\n","print(df['len_sentence'].min())"],"execution_count":13,"outputs":[{"output_type":"stream","text":["254.0\n","2.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TJBrNwLrpwst","executionInfo":{"status":"ok","timestamp":1606311274113,"user_tz":-210,"elapsed":42807,"user":{"displayName":"Hamoon jafarian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQqWT2-N8jq2Rj1MiVOscMiLQtxuqgyz6IZJcxPQ=s64","userId":"16895573338538093748"}}},"source":["df = df.drop(columns='len_sentence')"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p0fayvuPJMOZ"},"source":["### Train-Test split"]},{"cell_type":"code","metadata":{"id":"pOdabIOC1Jph","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606311274115,"user_tz":-210,"elapsed":42804,"user":{"displayName":"Hamoon jafarian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQqWT2-N8jq2Rj1MiVOscMiLQtxuqgyz6IZJcxPQ=s64","userId":"16895573338538093748"}},"outputId":"f1a5d232-a1fd-4964-e65d-a9c2ead743d9"},"source":["# Create a 85-15 train-test split.\n","import numpy as np\n","df_train, df_test = np.split(df.sample(frac=1, random_state=4040), [int(.85*len(df))])\n","print('{:>5,} training samples'.format(len(df_train)))\n","print('{:>5,} test samples'.format(len(df_test)))"],"execution_count":15,"outputs":[{"output_type":"stream","text":["8,290 training samples\n","1,463 test samples\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VpqKTkVmJdoU"},"source":["### Train data preparation"]},{"cell_type":"code","metadata":{"id":"D0g9q0pf9FsO","colab":{"base_uri":"https://localhost:8080/","height":225},"executionInfo":{"status":"ok","timestamp":1606311274118,"user_tz":-210,"elapsed":42800,"user":{"displayName":"Hamoon jafarian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQqWT2-N8jq2Rj1MiVOscMiLQtxuqgyz6IZJcxPQ=s64","userId":"16895573338538093748"}},"outputId":"982d31d8-80a1-48cc-c6e3-193fb1661621"},"source":["df_train['polarity'] = df_train['polarity'].replace(['positive', 'negative', 'neutral'], [0, 1, 2])\n","df_train = df_train.reset_index(drop=True)\n","\n","comments_a = df_train['sentence'].tolist()\n","comments_b = df_train['aspect'].tolist()\n","labels = df_train['polarity'].tolist()\n","\n","num_labels = 3\n","\n","print(df_train.shape)\n","df_train.head(5)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["(8290, 3)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","      <th>aspect</th>\n","      <th>polarity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>من اینو حدودا دوماهی هست گرفتم. اینترنت نامحدو...</td>\n","      <td>سرعت دانلود</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>دنبال یه تبلت برند با قیمت ارزان بودم که اینو ...</td>\n","      <td>کیفیت اسپیکر</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ایرفون بسیار خوش ساخت و با کیفیتی هستش و ازش ک...</td>\n","      <td>وان مور</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>شمابه چی این دلتونو خوش کردین نه یک باتری درست...</td>\n","      <td>باطری</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>من از روسیه این رو خریدم که ۱۰۰ میل شد حدود ۹۰...</td>\n","      <td>روسیه</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            sentence        aspect  polarity\n","0  من اینو حدودا دوماهی هست گرفتم. اینترنت نامحدو...   سرعت دانلود         0\n","1  دنبال یه تبلت برند با قیمت ارزان بودم که اینو ...  کیفیت اسپیکر         2\n","2  ایرفون بسیار خوش ساخت و با کیفیتی هستش و ازش ک...       وان مور         0\n","3  شمابه چی این دلتونو خوش کردین نه یک باتری درست...         باطری         0\n","4  من از روسیه این رو خریدم که ۱۰۰ میل شد حدود ۹۰...         روسیه         2"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"wFEZYInypVlM"},"source":["### Using Colab GPU for Training"]},{"cell_type":"code","metadata":{"id":"F4QEeZLhpVlU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606311287294,"user_tz":-210,"elapsed":55968,"user":{"displayName":"Hamoon jafarian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQqWT2-N8jq2Rj1MiVOscMiLQtxuqgyz6IZJcxPQ=s64","userId":"16895573338538093748"}},"outputId":"839bbcb9-4f75-44e1-ae74-a2bdcaaad17c"},"source":["import tensorflow as tf\n","\n","# Get the GPU device name.\n","device_name = tf.test.gpu_device_name()\n","\n","# The device name should look like the following:\n","if device_name == '/device:GPU:0':\n","    print('Found GPU at: {}'.format(device_name))\n","else:\n","    raise SystemError('GPU device not found')\n","\n","import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n","There are 1 GPU(s) available.\n","We will use the GPU: Tesla K80\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7jc8PyWIpVnW"},"source":["### Encoding the data"]},{"cell_type":"code","metadata":{"id":"ifygiBOppVnY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606311301680,"user_tz":-210,"elapsed":70348,"user":{"displayName":"Hamoon jafarian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQqWT2-N8jq2Rj1MiVOscMiLQtxuqgyz6IZJcxPQ=s64","userId":"16895573338538093748"}},"outputId":"547522ff-1ca2-48f9-feb9-7070c6ccf863"},"source":["# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","attention_masks = []\n","token_type_ids = []\n","\n","# For every sentence pair...\n","for (comment_a, comment_b) in zip(comments_a, comments_b):\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens.\n","    encoded_dict = tokenizer.encode_plus(\n","                        comment_a,           # Sentence to encode.\n","                        comment_b,           # Sentence to encode.\n","                        truncation = True,\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 270,           # Pad & truncate all sentences.\n","                        padding='max_length',\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                        return_token_type_ids = True,\n","                   )\n","    # Add the encoded sentence to the list.    \n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","    # And its token_type_ids    \n","    token_type_ids.append(encoded_dict['token_type_ids'])\n","\n","# Convert the lists into tensors.\n","input_ids = torch.cat(input_ids, dim=0)\n","token_type_ids = torch.cat(token_type_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels)\n","\n","\n","# Print sentence 0, now as a list of IDs.\n","print('comment_a: ', comments_a[0])\n","print('comment_b: ', comments_b[0])\n","print('Token IDs:', input_ids[0])\n","print('Token Type IDs:', token_type_ids[0])\n","print('Token Attention Mask:', attention_masks[0])"],"execution_count":18,"outputs":[{"output_type":"stream","text":["comment_a:  من اینو حدودا دوماهی هست گرفتم. اینترنت نامحدود همراه اول گرفتم از سایت ۱ شب تا ۱۱ صبح خونه که هستم سرعت دانلود تا ۵ مگ میرسه و سرکارم که میارم اینجا ۴٫۵ هست، سرعت دانلود تا ۱۴ مگ میرسه خیلی راحت. البته داخل خودتنظیمات مودم بری نوشته ۴G ولی اینجایی که من هستم ۴٫۵G هست و سرعتش فوق العاده عالیه. برنامه موبایلشو هم نصب کنید به راحتی میتونید از طریق خود برنامش شارژ کنید سیم کارتتون. بعضی از دوستان نوشته بودن داغ میکنه، ولی واسه من اصلا داغ نمیکنه به هیچ وج اکثرا هم من به پاور بانک میزنم استفاده میکنم \n","comment_b:  سرعت دانلود\n","Token IDs: tensor([    2,  2842, 25021, 10563, 32848,  2003,  2952, 11038,  1012,  4900,\n","        13887,  3287,  3067, 11038,  2791,  4394,  1455,  3256,  2848,  3797,\n","         5072, 19328,  2800,  6794,  4089,  7510,  2848,  1459,  6333, 34885,\n","         1379, 20785,  2015,  2800, 82434,  4938,  1458,  1393,  1459,  2952,\n","         1348,  4089,  7510,  2848,  4012,  6333, 34885,  3805,  6309,  1012,\n","         3546,  4127, 14587,  7380,  7786,  2009, 17157,  7916,  4196, 19019,\n","         3362, 29121,  2003,  2800,  2842,  6794,  1458,  1393, 12866,  2952,\n","         1379, 43218,  4692,  8505, 19142,  1012,  3329, 64051,  2005,  2820,\n","         5596,  3116,  2789,  5842, 21280,  2791,  3626,  2847, 26698,  2014,\n","         7450,  3116,  4301,  5108,  4957,  1012,  4322,  2791,  5440,  4196,\n","         3798,  7742, 12702,  1348,  3362, 29197,  2842,  5899,  7742, 38999,\n","         2789,  3371,  9289, 12313,  2820,  2842,  2789, 12199,  3227, 20128,\n","         2988,  5165,     4,  4089,  7510,     4,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n","Token Type IDs: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0])\n","Token Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bWO00wjPpVnq"},"source":["### Import the BERT model"]},{"cell_type":"code","metadata":{"id":"mWAW4E2ApVns","colab":{"base_uri":"https://localhost:8080/","height":179,"referenced_widgets":["4fb7a40109734a89b25c620bbe8fccad","3af284c1ea7e4fd8a0a12b556d3cc38d","45fd2eba7ce1456e97e8adfc5b2e4f13","dec1b0a142354178a111864c971b6213","8f14fe9d3ceb4b39a4033728aa28f6c4","d53f53b5586248d0bb1f46b58f316d77","c21cf7403fc8485fa9b70366799c21ec","eb6be1a585304bdb9fed0e2901afdb9e"]},"executionInfo":{"status":"ok","timestamp":1606311375991,"user_tz":-210,"elapsed":144651,"user":{"displayName":"Hamoon jafarian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQqWT2-N8jq2Rj1MiVOscMiLQtxuqgyz6IZJcxPQ=s64","userId":"16895573338538093748"}},"outputId":"6dc2e356-058e-4cc4-dbb5-cbb9598a02e9"},"source":["from transformers import AutoModelForSequenceClassification, AutoConfig, AdamW\n","# Using Pars-BERT model\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    \"HooshvareLab/bert-fa-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n","    num_labels = num_labels, # The number of output labels--2 for binary classification.\n","                    # You can increase this for multi-class tasks.   \n","    output_attentions = False, # Whether the model returns attentions weights.\n","    output_hidden_states = False, # Whether the model returns all hidden-states.\n","    hidden_dropout_prob=0.1, \n",")\n","config = AutoConfig.from_pretrained(\"HooshvareLab/bert-fa-base-uncased\")\n","\n","# Using Multilingual BERT model\n","# model = AutoModelForSequenceClassification.from_pretrained(\n","#     \"bert-base-multilingual-cased\", # Use the 12-layer BERT model, with an uncased vocab.\n","#     num_labels = num_labels, # The number of output labels--2 for binary classification.\n","#                     # You can increase this for multi-class tasks.   \n","#     output_attentions = False, # Whether the model returns attentions weights.\n","#     output_hidden_states = False, # Whether the model returns all hidden-states.\n","#     hidden_dropout_prob=0.1, \n","# )\n","# config = AutoConfig.from_pretrained(\"bert-base-multilingual-cased\")\n","\n","# Tell pytorch to run this model on the GPU.\n","if device.type == 'cuda':\n","  model.cuda()"],"execution_count":19,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4fb7a40109734a89b25c620bbe8fccad","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=654226731.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at HooshvareLab/bert-fa-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at HooshvareLab/bert-fa-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"JtZFsyPApVn6"},"source":["### Combine the training inputs into a TensorDataset"]},{"cell_type":"code","metadata":{"id":"yEXIxeF_pVn8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606311375997,"user_tz":-210,"elapsed":144651,"user":{"displayName":"Hamoon jafarian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQqWT2-N8jq2Rj1MiVOscMiLQtxuqgyz6IZJcxPQ=s64","userId":"16895573338538093748"}},"outputId":"cab88f29-cead-4e63-c5b0-51c46a69a698"},"source":["from torch.utils.data import TensorDataset\n","train_dataset = TensorDataset(input_ids, token_type_ids, attention_masks, labels)\n","print('{:>5,} training samples'.format(len(train_dataset)))"],"execution_count":20,"outputs":[{"output_type":"stream","text":["8,290 training samples\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xRQ0wh9vpVoK"},"source":["### Create dataloaders"]},{"cell_type":"code","metadata":{"id":"agDh5JOJpVoN","executionInfo":{"status":"ok","timestamp":1606311375999,"user_tz":-210,"elapsed":144646,"user":{"displayName":"Hamoon jafarian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQqWT2-N8jq2Rj1MiVOscMiLQtxuqgyz6IZJcxPQ=s64","userId":"16895573338538093748"}}},"source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","# The DataLoader needs to know our batch size for training, so we specify it \n","# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n","# size of 16 or 32.\n","batch_size = 16\n","\n","# Create the DataLoaders for our training and validation sets.\n","# We'll take training samples in random order. \n","train_dataloader = DataLoader(\n","            train_dataset,  # The training samples.\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3XTgZhuHpVoZ"},"source":["### Create Optimizer, Scheduler, Accuracy, Elapsed time"]},{"cell_type":"code","metadata":{"id":"UqWOIapbpVob","executionInfo":{"status":"ok","timestamp":1606311376001,"user_tz":-210,"elapsed":144642,"user":{"displayName":"Hamoon jafarian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQqWT2-N8jq2Rj1MiVOscMiLQtxuqgyz6IZJcxPQ=s64","userId":"16895573338538093748"}}},"source":["# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n","# I believe the 'W' stands for 'Weight Decay fix\"\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )\n","from transformers import get_linear_schedule_with_warmup\n","\n","# Number of training epochs. The BERT authors recommend between 2 and 4. \n","# We chose to run for 4, but we'll see later that this may be over-fitting the\n","# training data.\n","epochs = 4\n","\n","# Total number of training steps is [number of batches] x [number of epochs]. \n","# (Note that this is not the same as the number of training samples).\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mj9aT4lApVoo","executionInfo":{"status":"ok","timestamp":1606311376003,"user_tz":-210,"elapsed":144641,"user":{"displayName":"Hamoon jafarian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQqWT2-N8jq2Rj1MiVOscMiLQtxuqgyz6IZJcxPQ=s64","userId":"16895573338538093748"}}},"source":["# Function to calculate accuracy\n","import numpy as np\n","\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uuje5yoTpVo1","executionInfo":{"status":"ok","timestamp":1606311376010,"user_tz":-210,"elapsed":144637,"user":{"displayName":"Hamoon jafarian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQqWT2-N8jq2Rj1MiVOscMiLQtxuqgyz6IZJcxPQ=s64","userId":"16895573338538093748"}}},"source":["# Functin for elapsed time\n","import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b_TbjLjepVpD"},"source":["### Training loop"]},{"cell_type":"code","metadata":{"id":"d2fsFR7ypVpE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606314328077,"user_tz":-210,"elapsed":3096698,"user":{"displayName":"Hamoon jafarian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQqWT2-N8jq2Rj1MiVOscMiLQtxuqgyz6IZJcxPQ=s64","userId":"16895573338538093748"}},"outputId":"ff93073f-e7f0-400f-cf12-7b543432f0ba"},"source":["import random\n","import numpy as np\n","\n","# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","\n","# Set the seed value all over the place to make this reproducible.\n","seed_val = 2020\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# We'll store a number of quantities such as training and validation loss, \n","# validation accuracy, and timings.\n","training_stats = []\n","\n","# Measure the total training time for the whole run.\n","total_t0 = time.time()\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_train_loss = 0\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","\n","        # Progress update every 40 batches.\n","        if step % 40 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: token type ids\n","        #   [2]: attention masks\n","        #   [3]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_token_type_ids =  batch[1].to(device)\n","        b_input_mask = batch[2].to(device)\n","        b_labels = batch[3].to(device)\n","\n","        # Always clear any previously calculated gradients before performing a\n","        # backward pass. PyTorch doesn't do this automatically because \n","        # accumulating the gradients is \"convenient while training RNNs\". \n","        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","        model.zero_grad()        \n","\n","        # Perform a forward pass (evaluate the model on this training batch).\n","        # The documentation for this `model` function is here: \n","        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","        # It returns different numbers of parameters depending on what arguments\n","        # arge given and what flags are set. For our useage here, it returns\n","        # the loss (because we provided labels) and the \"logits\"--the model\n","        # outputs prior to activation.\n","        loss, logits = model(b_input_ids, \n","                             token_type_ids=b_token_type_ids,\n","                             attention_mask=b_input_mask, \n","                             labels=b_labels)\n","\n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","        total_train_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        scheduler.step()\n","   \n","\n","    # Calculate the average loss over all of the batches.\n","    avg_train_loss = total_train_loss / len(train_dataloader)            \n","    \n","    # Measure how long this epoch took.\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(training_time))\n","        \n","\n","    # Record all statistics from this epoch.\n","    training_stats.append(\n","        {\n","            'epoch': epoch_i + 1,\n","            'Training Loss': avg_train_loss,\n","            'Training Time': training_time,\n","        }\n","    )\n","\n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"],"execution_count":25,"outputs":[{"output_type":"stream","text":["\n","======== Epoch 1 / 4 ========\n","Training...\n","  Batch    40  of    519.    Elapsed: 0:00:56.\n","  Batch    80  of    519.    Elapsed: 0:01:53.\n","  Batch   120  of    519.    Elapsed: 0:02:51.\n","  Batch   160  of    519.    Elapsed: 0:03:48.\n","  Batch   200  of    519.    Elapsed: 0:04:45.\n","  Batch   240  of    519.    Elapsed: 0:05:42.\n","  Batch   280  of    519.    Elapsed: 0:06:39.\n","  Batch   320  of    519.    Elapsed: 0:07:36.\n","  Batch   360  of    519.    Elapsed: 0:08:33.\n","  Batch   400  of    519.    Elapsed: 0:09:30.\n","  Batch   440  of    519.    Elapsed: 0:10:27.\n","  Batch   480  of    519.    Elapsed: 0:11:24.\n","\n","  Average training loss: 0.50\n","  Training epcoh took: 0:12:19\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","  Batch    40  of    519.    Elapsed: 0:00:57.\n","  Batch    80  of    519.    Elapsed: 0:01:54.\n","  Batch   120  of    519.    Elapsed: 0:02:51.\n","  Batch   160  of    519.    Elapsed: 0:03:48.\n","  Batch   200  of    519.    Elapsed: 0:04:45.\n","  Batch   240  of    519.    Elapsed: 0:05:42.\n","  Batch   280  of    519.    Elapsed: 0:06:39.\n","  Batch   320  of    519.    Elapsed: 0:07:36.\n","  Batch   360  of    519.    Elapsed: 0:08:33.\n","  Batch   400  of    519.    Elapsed: 0:09:30.\n","  Batch   440  of    519.    Elapsed: 0:10:27.\n","  Batch   480  of    519.    Elapsed: 0:11:24.\n","\n","  Average training loss: 0.23\n","  Training epcoh took: 0:12:18\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","  Batch    40  of    519.    Elapsed: 0:00:57.\n","  Batch    80  of    519.    Elapsed: 0:01:54.\n","  Batch   120  of    519.    Elapsed: 0:02:51.\n","  Batch   160  of    519.    Elapsed: 0:03:48.\n","  Batch   200  of    519.    Elapsed: 0:04:45.\n","  Batch   240  of    519.    Elapsed: 0:05:42.\n","  Batch   280  of    519.    Elapsed: 0:06:39.\n","  Batch   320  of    519.    Elapsed: 0:07:36.\n","  Batch   360  of    519.    Elapsed: 0:08:33.\n","  Batch   400  of    519.    Elapsed: 0:09:29.\n","  Batch   440  of    519.    Elapsed: 0:10:26.\n","  Batch   480  of    519.    Elapsed: 0:11:23.\n","\n","  Average training loss: 0.14\n","  Training epcoh took: 0:12:18\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","  Batch    40  of    519.    Elapsed: 0:00:57.\n","  Batch    80  of    519.    Elapsed: 0:01:54.\n","  Batch   120  of    519.    Elapsed: 0:02:51.\n","  Batch   160  of    519.    Elapsed: 0:03:48.\n","  Batch   200  of    519.    Elapsed: 0:04:44.\n","  Batch   240  of    519.    Elapsed: 0:05:41.\n","  Batch   280  of    519.    Elapsed: 0:06:38.\n","  Batch   320  of    519.    Elapsed: 0:07:35.\n","  Batch   360  of    519.    Elapsed: 0:08:32.\n","  Batch   400  of    519.    Elapsed: 0:09:29.\n","  Batch   440  of    519.    Elapsed: 0:10:25.\n","  Batch   480  of    519.    Elapsed: 0:11:22.\n","\n","  Average training loss: 0.08\n","  Training epcoh took: 0:12:17\n","\n","Training complete!\n","Total training took 0:49:12 (h:mm:ss)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CnY-iEQUpVpv"},"source":["### Preparing test data"]},{"cell_type":"code","metadata":{"id":"8n_H9piU1xAM","colab":{"base_uri":"https://localhost:8080/","height":225},"executionInfo":{"status":"ok","timestamp":1606314328088,"user_tz":-210,"elapsed":3096698,"user":{"displayName":"Hamoon jafarian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQqWT2-N8jq2Rj1MiVOscMiLQtxuqgyz6IZJcxPQ=s64","userId":"16895573338538093748"}},"outputId":"c01443b9-b84d-4dd7-d400-e06df29d23ec"},"source":["df_test['polarity'] = df_test['polarity'].replace(['positive', 'negative', 'neutral'], [0, 1, 2])\n","df_test = df_test.reset_index(drop=True)\n","\n","comments_a = df_test['sentence'].tolist()\n","comments_b = df_test['aspect'].tolist()\n","labels = df_test['polarity'].tolist()\n","\n","\n","print(df_test.shape)\n","df_test.head(5)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["(1463, 3)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","      <th>aspect</th>\n","      <th>polarity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>امروز ۳۲G رو بدون گارانتی خریدم و با هات اسپات...</td>\n","      <td>دوربین سلفی</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>گوشی باکیفت و عالییه.</td>\n","      <td>گوشی</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>من در پیشنهاد شگفت انگیز تهیهاش کردم و امروز ب...</td>\n","      <td>جنس چرم</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>من حدودا یه ماهه اینو خریدم و یه شارژر بدون اس...</td>\n","      <td>برد وایرلسشم</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>بزرگترین مشکلش برای من اینه زمان شارژش خیلی با...</td>\n","      <td>سخت افزار</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            sentence        aspect  polarity\n","0  امروز ۳۲G رو بدون گارانتی خریدم و با هات اسپات...   دوربین سلفی         1\n","1                             گوشی باکیفت و عالییه.           گوشی         0\n","2  من در پیشنهاد شگفت انگیز تهیهاش کردم و امروز ب...       جنس چرم         2\n","3  من حدودا یه ماهه اینو خریدم و یه شارژر بدون اس...  برد وایرلسشم         0\n","4  بزرگترین مشکلش برای من اینه زمان شارژش خیلی با...     سخت افزار         2"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"Fh-LqcW21xAx","executionInfo":{"status":"ok","timestamp":1606314330627,"user_tz":-210,"elapsed":3099228,"user":{"displayName":"Hamoon jafarian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQqWT2-N8jq2Rj1MiVOscMiLQtxuqgyz6IZJcxPQ=s64","userId":"16895573338538093748"}}},"source":["# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","attention_masks = []\n","token_type_ids = []\n","\n","# For every sentence...\n","for (comment_a, comment_b) in zip(comments_a, comments_b):\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens.\n","    encoded_dict = tokenizer.encode_plus(\n","                        comment_a,           # Sentence to encode.\n","                        comment_b,           # Sentence to encode.\n","                        truncation = True,\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 270,           # Pad & truncate all sentences.\n","                        padding='max_length',\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                        return_token_type_ids = True,\n","                   )\n","    # Add the encoded sentence to the list.    \n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","    # And its token_type_ids    \n","    token_type_ids.append(encoded_dict['token_type_ids'])\n","\n","# Convert the lists into tensors.\n","input_ids = torch.cat(input_ids, dim=0)\n","token_type_ids = torch.cat(token_type_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels)"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"N2iTr9dKpVp_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606314330636,"user_tz":-210,"elapsed":3099231,"user":{"displayName":"Hamoon jafarian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQqWT2-N8jq2Rj1MiVOscMiLQtxuqgyz6IZJcxPQ=s64","userId":"16895573338538093748"}},"outputId":"9418a0d0-20fc-4c21-a719-a1b9c1dcfb5a"},"source":["# Combine the training inputs into a TensorDataset.\n","test_dataset = TensorDataset(input_ids, token_type_ids, attention_masks, labels)\n","print('{:>5,} test samples'.format(len(test_dataset)))\n","# Set the batch size.  \n","batch_size = 32 \n","# Create the DataLoader.\n","prediction_data = test_dataset\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"],"execution_count":28,"outputs":[{"output_type":"stream","text":["1,463 test samples\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZX8piI00pVqH"},"source":["### Model Evaluation on Test Data"]},{"cell_type":"code","metadata":{"id":"te5zUaInpVqJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606314376470,"user_tz":-210,"elapsed":3145058,"user":{"displayName":"Hamoon jafarian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQqWT2-N8jq2Rj1MiVOscMiLQtxuqgyz6IZJcxPQ=s64","userId":"16895573338538093748"}},"outputId":"71febde8-1e24-4855-9dbc-9e1647f2ed05"},"source":["# Prediction on test set\n","print('Predicting labels for {:,} test sentences...'.format(len(prediction_data)))\n","\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predictions , true_labels = [], []\n","\n","total_eval_accuracy=0\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  \n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_token_type, b_input_mask, b_labels = batch\n","  \n","  # Telling the model not to compute or store gradients, saving memory and \n","  # speeding up prediction\n","  with torch.no_grad():\n","      # Forward pass, calculate logit predictions\n","      outputs = model(b_input_ids, token_type_ids= b_token_type, \n","                      attention_mask=b_input_mask)\n","\n","  logits = outputs[0]\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)\n","  total_eval_accuracy += flat_accuracy(logits, label_ids)\n","\n","avg_val_accuracy = total_eval_accuracy / len(prediction_dataloader)\n","print(\"  Accuracy: {0:.3f}\".format(avg_val_accuracy))"],"execution_count":29,"outputs":[{"output_type":"stream","text":["Predicting labels for 1,463 test sentences...\n","  Accuracy: 0.903\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XEfQuC1a8Xr9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606314376479,"user_tz":-210,"elapsed":3145061,"user":{"displayName":"Hamoon jafarian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgQqWT2-N8jq2Rj1MiVOscMiLQtxuqgyz6IZJcxPQ=s64","userId":"16895573338538093748"}},"outputId":"03e2331d-6ed4-4d6f-d274-5e1c64295160"},"source":["# Calculating Macro F1\n","from sklearn import metrics\n","preds = [j for i in predictions for j in i]\n","preds_flat = np.argmax(preds, axis=1).flatten()\n","labels = [j for i in true_labels for j in i]\n","f1_score_micro = metrics.f1_score(labels, preds_flat, average='macro')\n","print(\"  f1_score: {0:.3f}\".format(f1_score_micro))"],"execution_count":30,"outputs":[{"output_type":"stream","text":["  f1_score: 0.893\n"],"name":"stdout"}]}]}